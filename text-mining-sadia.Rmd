---
title: "Text Analyis"
output: github_document
---




# Your mission

Perform text analysis.

## Okay, I need more information

Perform sentiment analysis or topic modeling using text analysis methods as demonstrated in the pre-class work and in the readings.

## Okay, I need even more information.

Do the above. Can't think of a data source?

- `gutenbergr`
- `AssociatedPress` from the `topicmodels` package
- `NYTimes` or `USCongress` from the `RTextTools` package
- Harry Potter Complete 7 Books text
    ```
    if (packageVersion("devtools") < 1.6) {
  install.packages("devtools")
}

devtools::install_github("bradleyboehmke/harrypotter")
    ```
- [State of the Union speeches](https://pradeepadhokshaja.wordpress.com/2017/03/31/scraping-the-web-for-presdential-inaugural-addresses-using-rvest/)
- Scrape tweets using [`twitteR`](https://www.credera.com/blog/business-intelligence/twitter-analytics-using-r-part-1-extract-tweets/)

Analyze the text for sentiment OR topic. **You do not need to do both**. The datacamp courses and [Tidy Text Mining with R](http://tidytextmining.com/) are good starting points for templates to perform this type of analysis, but feel free to *expand beyond these examples*.

```{r}
library(gutenbergr)
library(dplyr)
library(tidytext)
library(stringr)
gutenberg_metadata
data("gutenbergr", package="gutenbergr")

dt<-gutenberg_works(author == "Carroll, Lewis") #df that contains the books by authors

alice <- gutenberg_download(751) #download the books by book id in a dataframe

## cleaning the data:
alice_tidy<- alice %>%  
            mutate(linenumber = row_number(), chapter = cumsum(str_detect(text, regex("chapter|CHAPTER [\\divxlc]", ignore_case = TRUE)))) %>% 
  filter(chapter >0) %>% unnest_tokens(word, text)

#Questions to answer: 1. How does the proportion of negative and positive sentiments changes throughout the chapters? 2. what is the total proportion of positive and negative sentiments 3. top 10 positive and negative words

```

```{r}
#joining with the sentiment
alice_total<-alice_tidy %>% group_by(chapter) %>% count() %>% rename(total_words=n) #counting the total number of words in each chapter
alice_tidy_total<-alice_tidy %>% left_join(alice_total, by="chapter") #joining the count column with the alice_tidy data
alice_tidy_bing<-alice_tidy_total %>% inner_join(get_sentiments("bing")) #joining the sentiment with the tidy data with total column

#what are the top 10 positive and negative words?
positive_10<-alice_tidy_bing %>% count(word, sentiment) %>% arrange(desc(n)) %>% filter(sentiment == "positive") %>% top_n(10)
negative_10<-alice_tidy_bing %>% count(word, sentiment) %>% arrange(desc(n)) %>% filter(sentiment == "negative") %>% top_n(10)



#Total length of each chapter:
alice_tidy_bing %>% count(chapter) %>% arrange(desc(n)) #chapter 13 has much fewer words than other chapters 
alice_tidy_total %>% count(chapter) %>% arrange(desc(n))

```

```{r}

#graph to show the distribution of positive and negative words by each chapter
alice_tidy_bing %>% group_by(chapter) %>% count(chapter, sentiment) %>% rename(total_words=n) %>%
#Pyramid plot of each chapter by positive and negative words (word frequency by each chapter):
  ggplot(aes(x = as.factor(chapter), fill = sentiment)) + 
  geom_bar(data=filter(alice_tidy_bing, sentiment == "positive")) + 
  geom_bar(data=filter(alice_tidy_bing, sentiment == "negative"),aes(y=..count..*(-1))) + 
  scale_y_continuous(breaks=seq(-350,350,50),labels=abs(seq(-350,350,50))) + 
  coord_flip()

```


```{r} 
#What percent of positive words each chapters have?
alice_tidy_positive<-alice_tidy_bing %>% 
   count(chapter, sentiment, total_words) %>% #counts total positive and negative words in each chapter and keeps the total_words column (otherwise it wont be there)
   mutate(percent=n/total_words) %>%
   filter(sentiment == "positive") %>%
  arrange(desc(percent)) # the most positive chapters were 13, 1, 10, 2, 11. So, may be the book started in a positive way and have a happy ending

#What percent of negative words each chapters have?
alice_tidy_negative<-alice_tidy_bing %>% 
   count(chapter, sentiment, total_words) %>% #counts total positive and negative words in each chapter and keeps the total_words column (otherwise it wont be there)
   mutate(percent=n/total_words) %>%
   filter(sentiment == "negative") %>%
  arrange(desc(percent)) # the most negative chapters were 8,5,2,12,1. There might be something problem happening in the middle of the book. chapter 1 and 2 looks like to have high percent of positive and negative sentiments. Chapter 13 have the lowest percent of negative words indicating a happy ending


```

```{r}
#ggplot of sentiments throughout the chapter
library(ggplot2)
alice_tidy_bing %>% 
    # Filter for positive and negative words
    filter(sentiment %in% c("positive", "negative")) %>%
    # Count by date, sentiment, and total_words
    count(chapter, sentiment, total_words) %>%
    ungroup() %>%
    mutate(percent = n / total_words) %>%
    # Set up the plot with aes()
    ggplot(aes(chapter, percent, color=sentiment)) +
    geom_line(size = 1.5) +
    geom_smooth(method = "lm", se = FALSE, lty = 2) +
    expand_limits(y = 0)
# it looks like positive words had an abrupt increase at the end of the story. 

```


```{r}
#To do: make function to repeat with the different lexicons: nrc, afinn


```

# Timelines and Task


We will spend the next 2 weeks working on analyzing textual data in R. You will do the following:

- Start with some text based data.
- Clean data and prepare it for analysis
- Ask questions about the data
- Answer these questions with the data using tables and graphics
- Each group member must have their own unique question that they code the answer for. 

