---
title: "Text Analyis"
output: github_document
---




# Your mission

Perform text analysis.

## Okay, I need more information

Perform sentiment analysis or topic modeling using text analysis methods as demonstrated in the pre-class work and in the readings.

## Okay, I need even more information.

Do the above. Can't think of a data source?

- `gutenbergr`
- `AssociatedPress` from the `topicmodels` package
- `NYTimes` or `USCongress` from the `RTextTools` package
- Harry Potter Complete 7 Books text
    ```
    if (packageVersion("devtools") < 1.6) {
  install.packages("devtools")
}

devtools::install_github("bradleyboehmke/harrypotter")
    ```
- [State of the Union speeches](https://pradeepadhokshaja.wordpress.com/2017/03/31/scraping-the-web-for-presdential-inaugural-addresses-using-rvest/)
- Scrape tweets using [`twitteR`](https://www.credera.com/blog/business-intelligence/twitter-analytics-using-r-part-1-extract-tweets/)

Analyze the text for sentiment OR topic. **You do not need to do both**. The datacamp courses and [Tidy Text Mining with R](http://tidytextmining.com/) are good starting points for templates to perform this type of analysis, but feel free to *expand beyond these examples*.

# Timelines and Task

```{r}
library(RTextTools)
library(dplyr)
library(gutenbergr)
gutenberg_metadata
data("gutenbergr", package="gutenbergr")

picture<- gutenberg_download(174)
alice <- gutenberg_download(751)
Buddhism<- gutenberg_download(18223)
Bible<- gutenberg_download(10)
Koran<- gutenberg_download(2800)
```

```{r}
#Data Cleaning of picture
library(tidytext)
library(stringr)

picture.clean<- picture %>%  mutate(linenumber = row_number(), chapter = cumsum(str_detect(text, regex("chapter [\\divxlc]", ignore_case = TRUE)))) %>% filter(chapter >0) %>% unnest_tokens(word, text)
```


```{r}
#Clean Group Books
Bible1<-Bible %>% unnest_tokens(word, text)
Buddhism1<- Buddhism %>% unnest_tokens(word, text)
Koran1<-Koran %>% unnest_tokens(word, text)
alice_tidy<- alice %>%  mutate(linenumber = row_number(), chapter = cumsum(str_detect(text, regex("chapter|CHAPTER [\\divxlc]", ignore_case = TRUE)))) %>% #breaks down the book by chapter and stores the line number
  filter(chapter >0) %>% unnest_tokens(word, text)
```

```{r}
#Extracting Data
positive.words <-picture.clean %>% inner_join(get_sentiments("bing")) %>% filter(sentiment=="positive") %>% group_by(chapter) %>% top_n(10, word)
positive.words
negative.words <-picture.clean %>% inner_join(get_sentiments("bing")) %>% filter(sentiment=="negative") %>% group_by(chapter) %>% top_n(10, word)
negative.words
pos.neg<- picture.clean %>% inner_join(get_sentiments("bing")) %>% group_by(chapter) %>% count(chapter, sentiment) %>% rename(total_words=n)
Chapter.score<- picture.clean %>% inner_join(get_sentiments("afinn")) %>% group_by(chapter) %>% summarise(chapter.score=sum(score)) 
```

```{r}
#Group Comparison
#Group Extraction
Dorien.score<-picture.clean %>% inner_join(get_sentiments("afinn")) %>% summarise(score_per_word=sum(score)/nrow(picture.clean))
Bible_score<-Bible1 %>% inner_join(get_sentiments("afinn"))  %>% summarise(score_per_word=sum(score)/nrow(Bible1))
Buddhism_score<-Buddhism1 %>% inner_join(get_sentiments("afinn"))  %>% summarise(score_per_word=sum(score)/nrow(Buddhism1))
Koran_score<-Koran1 %>% inner_join(get_sentiments("afinn"))  %>% summarise(score_per_word=sum(score)/nrow(Koran1))
Alice_score<-alice_tidy %>% inner_join(get_sentiments("afinn"))  %>% summarise(score_per_word=sum(score)/nrow(alice_tidy))

Bible_total<-Bible1 %>% inner_join(get_sentiments("afinn")) %>% summarise(total= sum(score))
Dorien.total<-picture.clean %>% inner_join(get_sentiments("afinn")) %>% summarise(total= sum(score))
Buddhism_total<-Buddhism1 %>% inner_join(get_sentiments("afinn")) %>% summarise(total= sum(score))
Koran_total<-Koran1 %>% inner_join(get_sentiments("afinn")) %>% summarise(total= sum(score))
Alice_total<-alice_tidy %>% inner_join(get_sentiments("afinn")) %>% summarise(total= sum(score))

score<-rbind(Bible_score,Buddhism_score, Koran_score, Dorien.score, Alice_score) %>% mutate(book=c("Bible","The Essence of Buddhism","Koran", "Picture of Dorian Gray", "Alice's Adventures in Wonderland"))
score<- score%>% arrange(desc(score_per_word))
total<-rbind(Bible_total,Buddhism_total, Koran_total, Dorien.total, Alice_total) %>% mutate(book=c("Bible","The Essence of Buddhism","Koran", "Picture of Dorian Gray", "Alice's Adventures in Wonderland")) 
total<- total%>% arrange(desc(total))
ggplot(score,aes(x=book,y=score_per_word,fill=book)) + theme_classic() +geom_bar(stat = "identity", show.legend = F)  + xlab("")+ ylab("Score Per Word") + ggtitle("Sentiment Score Per Word Comparison") +coord_flip()
ggplot(total,aes(x=book,y=total,fill=book)) + theme_classic() +geom_bar(stat = "identity", show.legend = FALSE)  + xlab("")+ ylab("Score") + ggtitle("Total Score Book Comparison") +coord_flip()
```

```{r}
library(ggplot2)
#how does the character sentiments change per chapter? 
#use a time scale
# total amount of positive or negative words?
chapter.sentiment<- picture.clean %>% inner_join((get_sentiments("nrc"))) %>% group_by(chapter)%>% count(sentiment)
ggplot(chapter.sentiment, aes(x=sentiment, y=n))+ theme_classic() + geom_point(aes(color=sentiment), show.legend = F)+ggtitle("Top 10 Sentiment per Chapter \nThe Portrait of Dorian Grey") + ylab("Number")

ggplot(pos.neg, aes(x = chapter, y=total_words)) + theme_classic()+
  geom_col(aes(fill=sentiment)) +scale_fill_manual("legend", values = c("goldenrod3","cadetblue")) + ggtitle("Overall Chapter Sentiments") +ylab("Word Count")

 ggplot(pos.neg, aes(chapter, total_words, color=sentiment)) + theme_classic()+geom_line(size = 1.5) +
    geom_smooth(method = "lm", se = FALSE, lty = 2) + ylab("Number")+ ggtitle("Dichotomized Sentiment Per Chapter")+
   expand_limits(y = 0)
 
Dorien.score<-picture.clean %>% inner_join(get_sentiments("afinn")) %>% summarize(score_per_word=sum(score)/nrow(Bible1))

```

```{r}
#Comparing all three books
# using "afinn" sentiment, which book is the most positive or the most negative?
# will need to sum all words. 

We will spend the next 2 weeks working on analyzing textual data in R. You will do the following:

- Start with some text based data.
- Clean data and prepare it for analysis
- Ask questions about the data
- Answer these questions with the data using tables and graphics
- Each group member must have their own unique question that they code the answer for. 

